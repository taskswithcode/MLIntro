{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DimensionsExplained.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taskswithcode/MLIntro/blob/main/notebooks/DimensionsExplained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the notebook for the video\n",
        "- [What are the two meanings of 'Dimension' in Machine learning?](https://youtube.com/shorts/rPy8ZbNYoZk) [![Watch](https://raw.githubusercontent.com/taskswithcode/image_assets/main/.github/images/Watch.svg)](https://youtube.com/shorts/rPy8ZbNYoZk)\n",
        "- [Code walkthrough of this notebook](https://youtu.be/an0WeIGa8CQ) [![Code Walkthrough](https://raw.githubusercontent.com/taskswithcode/image_assets/main/.github/images/walkthrough.svg)](https://youtu.be/an0WeIGa8CQ)\n"
      ],
      "metadata": {
        "id": "fGa1CrF1lyMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will clarify the two distinct meanings of the word **Dimension** in the context of machine learning:\n",
        "\n",
        "1. The first usage of **dimension** refers to an attribute of the data structure used to store data, such as a tensor. A tensor is essentially a multidimensional matrix. The term 'dimension' in this context describes the number of indices required to locate an element within a tensor.\n",
        "2. The second usage of **dimension** refers to an important property of data itself, often used to describe data that is input into a machine learning model. Here, 'dimension' refers to the number of features that constitute the input data.\n",
        "\n",
        "_The same term **dimension** is used to describe both the data and the data structure that holds the data. However, the context in which the word is used clarifies its meaning. A few examples will illustrate this_\n"
      ],
      "metadata": {
        "id": "2PVhRppdQmdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So lets first download a dataset made up of grayscale image (shades of black) of handwritten digits. This dataset is typically used to train models to classify an image containing a digit.\n"
      ],
      "metadata": {
        "id": "WBbi4JFH-ElG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "ZBTFjBcx7db9",
        "outputId": "da889577-befc-41ad-cb17-070138bde334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets visualize a few of them"
      ],
      "metadata": {
        "id": "E2XGxrTL-8ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "images, labels = mnist.data.to_numpy(), mnist.target.to_numpy()\n",
        "\n",
        "# Reshape images to 28x28\n",
        "images = images.reshape(-1, 28, 28)\n",
        "\n",
        "# Selecting the first 4 images and their labels\n",
        "samples = images[:4]\n",
        "sample_labels = labels[:4]\n",
        "\n",
        "# Plotting the images\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(4):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    plt.imshow(samples[i], cmap='gray')\n",
        "    plt.title(f'Label: {sample_labels[i]}')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t9otjA0wZjTq",
        "outputId": "619d777f-eb3a-49de-d295-ea0cc8f68670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each image is stored in a two-dimensional tensor made up of 28 rows and 28 columns. Each element of this array is a pixel value that ranges from 0 through 255 where 0 represents black, 255 represents white with all numbers in between are shades of gray."
      ],
      "metadata": {
        "id": "u6S5Ds5B_CLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = mnist.data.to_numpy(), mnist.target.to_numpy()\n",
        "\n",
        "# Reshape images to 28x28\n",
        "images = images.reshape(-1, 28, 28)\n",
        "\n",
        "# Selecting a single image\n",
        "single_image = images[0]\n",
        "image_label = labels[0]\n",
        "\n",
        "# Print the tensor dimensions of the single image\n",
        "print(\"Tensor dimensions of the single image:\", single_image.shape)\n",
        "print(\"Label for this image:\",image_label)\n",
        "for i in range(28):\n",
        "  for j in range(28):\n",
        "    print(single_image[i][j],end=' ')\n",
        "  print()\n",
        "\n",
        "print(\"Dataset size:\",len(images))"
      ],
      "metadata": {
        "id": "t0UU8RaM76qO",
        "outputId": "d8ed7d4b-106c-4819-cfce-637c971a74e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor dimensions of the single image: (28, 28)\n",
            "Label for this image: 5\n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3.0 18.0 18.0 18.0 126.0 136.0 175.0 26.0 166.0 255.0 247.0 127.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 30.0 36.0 94.0 154.0 170.0 253.0 253.0 253.0 253.0 253.0 225.0 172.0 253.0 242.0 195.0 64.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 49.0 238.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 253.0 251.0 93.0 82.0 82.0 56.0 39.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 18.0 219.0 253.0 253.0 253.0 253.0 253.0 198.0 182.0 247.0 241.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 80.0 156.0 107.0 253.0 253.0 205.0 11.0 0.0 43.0 154.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 14.0 1.0 154.0 253.0 90.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 139.0 253.0 190.0 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 11.0 190.0 253.0 70.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 35.0 241.0 225.0 160.0 108.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 81.0 240.0 253.0 253.0 119.0 25.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 45.0 186.0 253.0 253.0 150.0 27.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 16.0 93.0 252.0 253.0 187.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 249.0 253.0 249.0 64.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 46.0 130.0 183.0 253.0 253.0 207.0 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 39.0 148.0 229.0 253.0 253.0 253.0 250.0 182.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 24.0 114.0 221.0 253.0 253.0 253.0 253.0 201.0 78.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 23.0 66.0 213.0 253.0 253.0 253.0 253.0 198.0 81.0 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 18.0 171.0 219.0 253.0 253.0 253.0 253.0 195.0 80.0 9.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 55.0 172.0 226.0 253.0 253.0 253.0 253.0 244.0 133.0 11.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 136.0 253.0 253.0 253.0 212.0 135.0 132.0 16.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \n",
            "Dataset size: 70000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned above, each image is stored in a 2-dimensional tensor. It is considered 2-dimensional because two indices are required to access each element within the tensor.\n",
        "\n",
        "Lets look at the print statement for the tensor holding a single image:\n",
        "\n",
        "**Tensor dimensions of the single image: (28, 28)**\n",
        "\n",
        "The output indicates that the shape of the tensor holding a single image is 2-dimensional. It also shows that there are 28 numbers along each dimension or axis. In the context of tensor shapes, each **dimension** is also referred to as an **axis**. For instance, tensor operations implemented in libraries like PyTorch or NumPy refer to each dimension as an axis."
      ],
      "metadata": {
        "id": "Lc9Z1VWrZr2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The second interpretation of 'dimension' refers to an aspect of the data itself, which in this case is the total number of individual pixels in a single image. This image has 784 pixels arranged in 28 rows and 28 columns. Each pixel represents a feature of the input. The phrase **\"dimensions of the feature space\"** refers to the total number of features of the input. Despite being simple grayscale images, this dataset is considered high-dimensional because it has 784 features.\n"
      ],
      "metadata": {
        "id": "72wsxb-3m-0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In summary, from a data structure or storage perspective, we only need a 2-dimensional tensor to store an image. However, this two-dimensional tensor contains high-dimensional data with 784 features where each pixel is a feature."
      ],
      "metadata": {
        "id": "TANwLHxwA1ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " _An implication of high-dimensional input data is that the number of possible inputs to a model can be large. For instance, the number of possible images that can be created with 784 features, where each feature can take on 256 values,  is 256^784 or  2^(8*784) which is 2^6272, a figure several orders of magnitude larger than the estimated number of atoms in the universe (approximately 2^266)._"
      ],
      "metadata": {
        "id": "JFZ7TLVp_cgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back to Tensors...**\n",
        "\n",
        "Tensors are the backbone data structure of machine learning – they are used to represent input, output, as well as model parameters. Couple of examples to get a feel for tensors."
      ],
      "metadata": {
        "id": "nb66Za6S_AHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Creating a two-dimensional array\n",
        "two_dim_array = np.array([[4, 5, 6],[7,8,9]]) #This tensor is 2 groups of 3 numbers\n",
        "\n",
        "# Printing the array\n",
        "print(\"Two-dimensional array:\")\n",
        "print(two_dim_array)\n",
        "\n",
        "# Printing the length of axis 0 (rows) and axis 1 (columns)\n",
        "length_axis_0 = two_dim_array.shape[0]  # Number of rows\n",
        "length_axis_1 = two_dim_array.shape[1]  # Number of columns\n",
        "\n",
        "print(\"\\nLength of axis 0 (rows):\", length_axis_0)\n",
        "print(\"Length of axis 1 (columns):\", length_axis_1)"
      ],
      "metadata": {
        "id": "atnFqID1_F55",
        "outputId": "3ca24ff3-eb1b-4808-b963-2e3ef3279a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two-dimensional array:\n",
            "[[4 5 6]\n",
            " [7 8 9]]\n",
            "\n",
            "Length of axis 0 (rows): 2\n",
            "Length of axis 1 (columns): 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[0.6733769,  0.74881172, 0.75416955, 0.13230142],\n",
        " [0.23387471, 0.43782733, 0.44806045, 0.75545152],\n",
        " [0.57339164, 0.01181629, 0.17665073, 0.24029428],\n",
        " [0.56351062, 0.89733891, 0.48641948, 0.42417642],\n",
        " [0.90163862, 0.97103787, 0.88086453, 0.51607751]]) #This tensor is 5 groups of 4 numbers\n",
        "\n",
        "print(x.shape)\n",
        "print(x.shape[0])\n",
        "print(x.shape[1])"
      ],
      "metadata": {
        "id": "VXrNleyl_K61",
        "outputId": "1c0bc477-7909-4c55-cf21-1387939a8c69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 4)\n",
            "5\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional notes:**\n",
        "\n",
        "This characteristic of the feature space in input modalities such as images, text, and audio, where the number of features is substantially large, is often referred to as the **curse of dimensionality**. A large number of dimensions is considered a curse due to the challenges they pose for training models on such inputs.\n",
        "\n",
        "- **Model Complexity**: To accurately model the distribution of high-dimensional data,in many cases, an extremely complex model with a vast number of parameters is required. This complexity makes the model challenging to train and increases the risk of overfitting.\n",
        "\n",
        "- **Data Sparsity**: Despite the immense number of potential images, the actual number of images available for training is finite and often relatively small compared to the space of all possible images. This results in sparsity in the data distribution, with many potential images not being represented in the training set. However, it is also important to note that not all potential images occur naturally. We can leverage this fact to ensure that the training data primarily represents images that naturally occur\n",
        "\n",
        "- **Computational Constraints**: Handling high-dimensional data demands significant computational resources. Training models on this data can be exceptionally resource-intensive and time-consuming.\n",
        "\n",
        "- **Assigning Probabilities**: Lastly, assigning probabilities to inputs in high-dimensional data is challenging. Generative models approximate the underlying probability distribution of the input data space, either directly or indirectly. They must learn, or at least approximate, this underlying probability distribution to generate new samples. It's important to note that the immense size of the feature space makes it practically impossible to assign probabilities to each possible image, necessitating the use of approximation methods."
      ],
      "metadata": {
        "id": "qUetoriVDLYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite these challenges, in some cases, it is possible to train even simple models to classify digits with high accuracy using the aforementioned dataset. Typically, this is achieved by training a model on a fraction of the 70,000 images and then testing it on the remaining fraction.\n",
        "\n",
        "In future notebooks, we will explore how to achieve high accuracy in classifying these digits. The models we'll examine are  neural networks, which perform quite well with **high-dimensional data** (in this context, 'dimension' refers to the feature space). They accomplish this by reducing the dimensionality of the input feature space, or in other words, by condensing features.\n",
        "\n",
        "*It is worth noting that there are approaches to classifying digits without using neural networks. However, they do not scale as well to handle large datasets with high-dimensional inputs compared to neural networks, even though they offer other advantages like interpretability and model simplicity.*"
      ],
      "metadata": {
        "id": "AUH6omMQECXl"
      }
    }
  ]
}